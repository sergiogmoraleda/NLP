{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- -> Stemming <- ----\n",
    "\n",
    " La derivación(stemming) es el proceso de reducir una palabra a su raíz que se afija a sufijos y prefijos o a las raíces de las palabras conocidas como lemas. Por ejemplo: palabras como “Likes”, “liked”, “likely” y “liking” se reducirán a “like” después de la derivación1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- -> Esta es la forma en la que Porter Stemmer hace 'stemming' de palabras. <- ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run -> run\n",
      "runner -> runner\n",
      "running -> run\n",
      "ran -> ran\n",
      "runs -> run\n",
      "easily -> easili\n",
      "fairly -> fairli\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Descargamos la librería Porter Stemmer de NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "palabras = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly']\n",
    "\n",
    "for p in palabras:\n",
    "    print(p + ' -> ' + p_stemmer.stem(p))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- -> Esta es la forma en la que Snowball Stemmer hace 'stemming' de palabras. <- ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#este derivador de palabras necesita de parametros como el lenguaje del texto para procesar las diferentes palabras\n",
    "\n",
    "s_stemmer = SnowballStemmer('english', False)\n",
    "\n",
    "palabras = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly']\n",
    "\n",
    "for p in palabras:\n",
    "    print(p + ' --> ' + s_stemmer.stem(p))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  -> Stemming con SpaCy y NLTK <- ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John \t PROPN \t John\n",
      "Adam \t PROPN \t Adam\n",
      "is \t AUX \t be\n",
      "one \t NUM \t one\n",
      "the \t DET \t the\n",
      "researcher \t NOUN \t researcher\n",
      "who \t PRON \t who\n",
      "invent \t VERB \t invent\n",
      "the \t DET \t the\n",
      "direction \t NOUN \t direction\n",
      "of \t ADP \t of\n",
      "way \t NOUN \t way\n",
      "towards \t ADP \t towards\n",
      "success \t NOUN \t success\n",
      "! \t PUNCT \t !\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "texto1 = nlp(u\"John Adam is one the researcher who invent the direction of way towards success!\")\n",
    "\n",
    "for token in texto1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrarLemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John         PROPN  John\n",
      "Adam         PROPN  Adam\n",
      "is           AUX    be\n",
      "one          NUM    one\n",
      "the          DET    the\n",
      "researcher   NOUN   researcher\n",
      "who          PRON   who\n",
      "invent       VERB   invent\n",
      "the          DET    the\n",
      "direction    NOUN   direction\n",
      "of           ADP    of\n",
      "way          NOUN   way\n",
      "towards      ADP    towards\n",
      "success      NOUN   success\n",
      "!            PUNCT  !\n"
     ]
    }
   ],
   "source": [
    "texto2 = nlp(u\"John Adam is one the researcher who invent the direction of way towards success!\")\n",
    "\n",
    "mostrarLemmas(texto2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\SGarc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "#Ahora  vamos a hacer Stemming con nltk Porter Stemmer y con Corpus stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones = nltk.sent_tokenize(parrafo)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(oraciones)):\n",
    "    palabras = nltk.word_tokenize(oraciones[i])\n",
    "    palabras = [stemmer.stem(palabra) for palabra in palabras if palabra not in set(stopwords.words('english'))]\n",
    "    oraciones[i] = ' '.join(palabras)\n",
    "\n",
    "oraciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----  -> Lemmatization con NLTK <- ----\n",
    "\n",
    "La lematización en NLP (Procesamiento de Lenguaje Natural) consiste en reducir las palabras a su forma base o canónica, conocida como lema. Esto se logra mediante el uso de un vocabulario y un análisis minucioso de la construcción de las palabras para eliminar únicamente las terminaciones inflexibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank much .',\n",
       " 'Thank Academy .',\n",
       " 'Thank room .',\n",
       " 'I congratulate incredible nominee year .',\n",
       " 'The Revenant product tireless effort unbelievable cast support leader around world speak big polluter , speak humanity , indigenous people world , billion billion underprivileged people would affected .',\n",
       " 'For child ’ child , people whose voice drowned politics greed .',\n",
       " 'I thank amazing award tonight .',\n",
       " 'Let u take planet granted .',\n",
       " 'I take tonight granted .',\n",
       " 'Thank much .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "parrafo = \"\"\" Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
    "\n",
    "oraciones = nltk.sent_tokenize(parrafo)\n",
    "lematizador = WordNetLemmatizer()\n",
    "\n",
    "#así lematizamos el parrafo\n",
    "\n",
    "for i in range(len(oraciones)):\n",
    "    palabras = nltk.word_tokenize(oraciones[i])\n",
    "    palabras = [lematizador.lemmatize(palabra) for palabra in palabras if palabra not in set(stopwords.words('english'))]\n",
    "    oraciones[i] = ' '.join(palabras)\n",
    "\n",
    "oraciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagámoslo en español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Estoy profundamente honrado agradecido haber sido elegido recibir prestigioso premio .',\n",
       " 'Quiero agradecer comité selección reconocer trabajo esfuerzo .',\n",
       " 'También quiero agradecer familia amigo apoyo constante momento .',\n",
       " 'Sin , aquí hoy .',\n",
       " 'Este premio gran logro motiva seguir trabajando duro alcanzar metas .',\n",
       " 'Espero poder seguir contribuyendo positivamente campo trabajo .',\n",
       " 'Una vez , muchas gracias honor .',\n",
       " 'Lo aprecio .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "parrafo = \"\"\" Estoy profundamente honrado y agradecido por haber sido elegido para recibir este prestigioso premio. \n",
    "    Quiero agradecer al comité de selección por reconocer mi trabajo y esfuerzo.\n",
    "    También quiero agradecer a mi familia y amigos por su apoyo constante en todo momento. Sin ellos, no estaría aquí hoy.\n",
    "    Este premio es un gran logro para mí y me motiva a seguir trabajando duro para alcanzar mis metas. Espero poder seguir contribuyendo positivamente en mi campo de trabajo.\n",
    "    Una vez más, muchas gracias por este honor. Lo aprecio mucho.\"\"\"\n",
    "\n",
    "oraciones = nltk.sent_tokenize(parrafo)\n",
    "lematizador = WordNetLemmatizer()\n",
    "\n",
    "# así lematizamos el parrafo\n",
    "\n",
    "for i in range(len(oraciones)):\n",
    "    palabras = nltk.word_tokenize(oraciones[i])\n",
    "    palabras = [lematizador.lemmatize(\n",
    "        palabra) for palabra in palabras if palabra not in set(stopwords.words('spanish'))]\n",
    "    oraciones[i] = ' '.join(palabras)\n",
    "\n",
    "oraciones\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f638b33ce15311dd8815655847e95bc4984b6e6a62164189425d9874633e0160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
